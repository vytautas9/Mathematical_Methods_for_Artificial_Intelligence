---
title: "Mathematical Methods for Artificial Intelligence Lab 3"
author: "Vytautas Kraujalis"
date: '2021-10-30'
output: 
  pdf_document:
    toc: true 
    toc_depth: 3
    number_sections: true
    highlight: tango
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Reading Data
```{r}
set.seed(123)

data_original <- read.csv("Arrhythmia_Dataset.csv")
```

# Required packages
```{r}
library(SmartEDA)
library(dplyr)
library(ggplot2)
library(caret)
library(rattle)
library(partykit)
library(groupdata2)
library(cvms)

# Function for 3 class accuracy from confusion matrix
classAcc <- function(confusionMatrix) {
  class0 <- round(confusionMatrix$table[1, 1] / sum(confusionMatrix$table[, 1]) * 100, 1)
  class1 <- round(confusionMatrix$table[2, 2] / sum(confusionMatrix$table[, 2]) * 100, 1)
  class2 <- round(confusionMatrix$table[3, 3] / sum(confusionMatrix$table[, 3]) * 100, 1)
  acc <- c(class0, class1, class2 )
  names(acc) <- colnames(confusionMatrix$table)
  return(acc)
}
```

# Parallel processing
```{r}
library(parallel) 
no_cores <- detectCores() - 1
library(doParallel)
cl <- makePSOCKcluster(no_cores)
registerDoParallel(cl)
```


# EDA, first look at the dataset
```{r}
ExpData(data_original,type=1)
```

We have a dataset of 175729 observations with 34 variables, of which are 2 text variables and 32 - numerical. All variables have no missing values.

Let's look at the text variables:
```{r}
data_original %>% 
   group_by(record) %>% 
   summarise(n = n()) %>% 
   summary()
```

Our first text variable record has 75 unique values (unique patients). Each patient has on average 2343 observations.

```{r}
data_original %>% 
   group_by(type) %>% 
   summarise(n = n()) %>% 
   mutate(n_prop = round(n / sum(n) * 100, 2))
```

The next text variable type is our target variable. It has 5 classes, but there's a huge class imbalance. 2 out of 5 classes takes up to `r 87.38 + 11.38`% of all observations, 1 class has just 6 observations. We are planning to combine F, Q, SVEB classes into one class, as those 3 classes combined only takes up to `r 0.12 + 1.11`% of observations.

N (Normal) - Normal beat.

SVEB (Supraventricular ectopic beat) - Supraventricular Ectopic Beats indicates atrial irritability.  Isolated Supraventricular Ectopic Beats are generally not significant in nature but a high frequency can represent more risk.  An increasing trend in Supraventricular Ectopic Beats may be an indicator or sign for atrial fibrillation. Atrial Fibrillation is considered to be significant as it can lead to heart attack or stroke.

VEB (Ventricular ectopic beat) - Ventricular ectopics are a type of arrhythmia or abnormal heart rhythm. It is caused by the electric signals in the heart starting in a different place and travelling a different way through the heart. If it happens occasionally, it should not cause any problems but if it happens a lot, you will need to have treatment.

F (Fusion beat) - A fusion beat occurs when a supraventricular and a ventricular impulse coincide to produce a hybrid complex. It indicates that there are two foci of pacemaker cells firing simultaneously: a supraventricular pacemaker (e.g. the sinus node) and a competing ventricular pacemaker (source of ventricular ectopics).

Q (Unknown beat) - Unknown beat.

```{r}
data <- data_original %>% 
   mutate(type = case_when(
   type %in% c("F", "Q", "SVEB") ~ "F_Q_SVEB",
   TRUE ~ type
   ))

data %>% 
   group_by(type) %>% 
   summarise(n = n()) %>% 
   mutate(n_prop = round(n / sum(n) * 100, 2))
```

We combined the 3 classes into 1 class, which has only 2183 observations (1.24%) which is still low, but still better for our basic models to classify.


We are not interested in the patient record variable, so we'll remove it:
```{r}
data <- data %>% 
   select(-record)
```

Let's look at descriptive statistics of each variable:
```{r}
ExpNumStat(data,by ="A",round= 2, gp = "type") %>% 
  select(Vname, min, max, mean, median, SD)

variables_of_further_interest <- c("X0_post.RR", "X0_pq_interval", "X0_pre.RR", "X0_qrs_interval", "X0_qt_interval", "X1_post.RR", "X1_pre.RR", "X1_qrs_interval", "X1_qt_interval")
```

We noticed some variables which should require further analysis, those variables are:
```{r}
variables_of_further_interest
```

We will look through these variables more closely and report any irregularities.


```{r}
ggplot(data, aes(x = X0_qrs_interval, color = type)) +
   geom_boxplot() +
   theme_minimal()

data_original %>% 
   filter(X0_qrs_interval > 125)
```

There's quite an unusual observation in X0_qrs_interval variable, where our new class F_Q_SVEB has a value of > 125. We also see from the original dataset that this observation was classified as F. We are going to classify this observation as an outlier and remove it.
```{r}
data <- data %>% 
   filter(X0_qrs_interval < 125)
```

```{r}
ggplot(data, aes(x = X0_qt_interval, color = type)) +
   geom_boxplot() +
   theme_minimal()
```

We noticed another outlier, for the X0_qt_interval variable, when the type is VEB, the value is close to 250 which is quite unusual in this dataset. We'll remove this observation.
```{r}
data <- data %>% 
   filter(X0_qt_interval < 225)
```

```{r}
ggplot(data, aes(x = X1_qrs_interval, color = type)) +
   geom_boxplot() +
   theme_minimal()

data_original %>% 
   filter(X1_qrs_interval > 75, type %in% c("F", "Q", "SVEB"))
```

Seems like another outlier was detected in X1_qrs_interval variable, where the type is F_Q_SVEB and the value is close to 100. From the original dataset, this observation had F type. We'll remove it.
```{r}
data %>% 
   mutate(row = row_number()) %>% 
   filter(X1_qrs_interval > 75, type == "F_Q_SVEB") %>% 
   select(row)

data <- data %>% 
   filter(row_number() != 10934)
```


We should look at the correlation between variables
```{r}
# Correlation

corr_simple <- function(df,sig=0.5){
  corr <- cor(df)
  #prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  corr[corr == 1] <- NA 
  #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr) 
  #select significant values  
  corr <- subset(corr, abs(Freq) > sig) 
  #sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),] 
  return(corr)
}

correlation_matrix = cor(data %>% select(-type))

length(findCorrelation(correlation_matrix, cutoff = 0.99))
length(findCorrelation(correlation_matrix, cutoff = 0.95))
length(findCorrelation(correlation_matrix, cutoff = 0.9))
```

We have `r length(findCorrelation(correlation_matrix, cutoff = 0.99))` variables with higher than .99 correlation.
We have `r length(findCorrelation(correlation_matrix, cutoff = 0.95))` variables with higher than .95 correlation.
We have `r length(findCorrelation(correlation_matrix, cutoff = 0.9))` variables with higher than .9 correlation.

# Pre-process
## Removal of correlated variables
We've found some highly correlated variables. We don't have a lot of variables (32) and because of that, we'll only remove the variables with >.99 correlation.
```{r}
data <- data %>% 
   select(-findCorrelation(correlation_matrix, cutoff = 0.99))
```

# Fitting models
## LDA
```{r}
fitControl = trainControl(## 5-fold CV
                           method = "cv",
                           number = 5,
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = multiClassSummary)
lda.fit = train(type ~ ., data = data, 
                 method = "lda",
                 preProcess=c("center","scale"),
                 trControl = fitControl)

print(lda.fit$results %>% t())

```

AUC is 0.96, which is pretty good, the Mean F1 is 0.8 which also indicated quite a good model. The variance for AUC is 0.002, for Mean F1 - 0.003, this indicated a stable model.


```{r}
confusion_matrix <- confusionMatrix(lda.fit)

confusion_matrix
classAcc(confusion_matrix)
```


The overall accuracy is pretty good (96.8%), the accuracies for each class are: F_Q_SVEB = 60.9%, N = 99.6%, VEB = 79.6%.

## QDA
```{r}
fitControl <- trainControl(# 5-fold CV
                           method = "cv",
                           number = 5,
                           # Estimate class probabilities
                           classProbs = TRUE,
                           # Evaluate performance using 
                           # the following function
                           summaryFunction = multiClassSummary,
                           verboseIter = TRUE)
# qda.fit <- train(type ~ ., data = data,
#                 method = "qda",
#                 preProcess=c("center","scale"),
#                 trControl = fitControl)
```

Can't fit a QDA model because of our small class F_Q_SVEB.

## RDA
```{r}
library(tictoc)
tic()
fitControl <- trainControl(## 5-fold CV
                           method = "cv",
                           number = 5,
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = multiClassSummary,
                           savePredictions="all",
                           verboseIter = TRUE)
rdaGrid =  expand.grid(lambda = seq(0.1,1,0.1), 
                        gamma = seq(0,1,0.1))
rda.fit <- train(type ~ ., data = data, 
                 method = "rda", 
                 trControl = fitControl,
                 preProcess=c("center", "scale","pca"),
                 ## Now specify the exact models 
                 ## to evaluate:
                 tuneGrid = rdaGrid)
toc()

trellis.par.set(caretTheme())
plot(rda.fit, metric = "AUC")
plot(rda.fit, metric = "AUC", plotType = "level")
```

From those 2 graphs, we can see that the best cross-validation AUC is achieved using gamma - 0 and Lambda 0.5 or 0.6.

```{r}
resamp = rda.fit$pred %>%
    filter(gamma == 0 & lambda == 0.6)
confusion_matrix <- confusionMatrix(resamp$pred, resamp$obs)
confusion_matrix
classAcc(confusion_matrix)
```
Overall accuracy using gamma - 0 and lambda - 0.6 is 95.4%, while accuracies for each class are: F_Q_SVEB = 60.1%, N = 97.8% and VEB = 80.8%.

## CART
```{r}
fitControl <- trainControl(## 5-fold CV
                           method = "cv",
                           number = 5,
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = multiClassSummary,
                           savePredictions="all",
                           verboseIter = TRUE)
cart.fit <- train(type ~ ., data = data, 
                 method = "rpart", 
                 trControl = fitControl,
                 preProcess=c("center", "scale"))

cart.fit
```

```{r}
confusion_matrix <- confusionMatrix(cart.fit)

confusion_matrix
classAcc(confusion_matrix)
```

Our overall accuracy is 95.9%, but if we would look at each class accuracies: F_Q_SVEB = 0%, N = 99.5% and VEB = 79.3% we could see that our newly created class has 0% accuracy.

```{r}
fancyRpartPlot(cart.fit$finalModel)
```

We can see from the final model, that our simple decision tree is too simple for our data.

## Conditional Inference Tree
```{r}
# Create 5 fold columns with 3 folds each
data_cv <- fold(
  data,
  k = 5,
  cat_col = "type",
  parallel = TRUE # set to TRUE to run in parallel
) %>% 
   mutate(type = as.factor(type))

data_cv %>% 
   count(.folds, type)
```

We created fold for 5-fold cross validation with somewhat equal distributions of target variable in each fold.

```{r}
ctree_model_fn <- function(train_data, formula, hyperparameters){
   partykit::ctree(formula = as.formula(formula), 
         data = train_data)
}

ctree_predict_fn <- function(test_data, model, formula, hyperparameters, train_data){
   stats::predict(object = model,
           newdata = test_data,
           allow.new.levels = TRUE,
           type = "prob")
}

formula <- paste("type ~ ", paste(colnames(data %>% select(-type)), collapse= "+"))

ctree.fit <- cross_validate_fn(
   data = data_cv,
   formulas = formula,
   type = "multinomial",
   model_fn = ctree_model_fn,
   predict_fn = ctree_predict_fn,
   fold_cols = ".folds",
   parallel = TRUE
)

```

```{r}
confusion_matrix_all <- ctree.fit$`Confusion Matrix`[[1]]

confusion_matrix <- confusion_matrix_all %>% 
   select(-`Fold Column`) %>% 
   mutate(N_perc = round(N / sum (N) * 100, 2)) %>% 
   select(-N) %>% 
   tidyr::pivot_wider(names_from = Target, values_from = N_perc) %>% 
   remove_rownames %>% 
   tibble::column_to_rownames(var = "Prediction")

class0 <- round(confusion_matrix[1, 1] / sum(confusion_matrix[, 1]) * 100, 1)
class1 <- round(confusion_matrix[2, 2] / sum(confusion_matrix[, 2]) * 100, 1)
class2 <- round(confusion_matrix[3, 3] / sum(confusion_matrix[, 3]) * 100, 1)
class_acc <- c(class0, class1, class2 )
names(class_acc) <- colnames(confusion_matrix)

overall_acc <- sum(diag(as.matrix(confusion_matrix)))

overall_acc
class_acc
```

Our conditional inference tree gave us overall accuracy of 99.15%, while the accuracies of each class are: F_Q_SVEB - 78.4%, N - 99.7%, VEB - 97.4%.

# Comparison of models
```{r}
resamps = resamples(list(LDA = lda.fit, RDA = rda.fit, CART = cart.fit))

accuracies <- data.frame() %>% 
   bind_rows(
      classAcc(confusionMatrix(lda.fit)),
      classAcc(confusionMatrix(resamp$pred, resamp$obs)),
      classAcc(confusionMatrix(cart.fit)),
      class_acc
             ) %>% 
   bind_cols(
      model = c("LDA", "RDA", "CART", "Conditional Inference Tree"),
      overall_accuracy = round(c(
         sum(diag(as.matrix(confusionMatrix(lda.fit)$table))),
         sum(diag(as.matrix(confusionMatrix(resamp$pred, resamp$obs)$table))) / nrow(data) * 100,
         sum(diag(as.matrix(confusionMatrix(cart.fit)$table))),
         overall_acc
      ), 2)
      ) %>% 
   tibble::column_to_rownames(var = "model")


summary(resamps)

accuracies
```

Conditional Inference Tree gave us the best result, the overall accuracy reaches 99.1%, while the accuracies for each class are: F_Q_SVEB = 78.4%, N = 99.7% and VEB = 97.4%.

Both LDA and RDA models gave very similar results, resulting in ~60% accuracy for F_Q_SVEB, ~99% for N and ~80% for VEB classes.

Decision tree performed worse, the model could not predict F_Q_SVEB class at all. Seems like the decision tree was too simple to capture our new class.


# References
https://cran.r-project.org/web/packages/cvms/vignettes/cross_validating_custom_functions.html
https://cran.r-project.org/web/packages/cvms/readme/README.html#main-functions
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4980381/




