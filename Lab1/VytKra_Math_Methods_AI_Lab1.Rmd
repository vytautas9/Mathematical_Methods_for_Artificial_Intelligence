---
title: "VytKra Mathematical Methods for Artificial Intelligence Lab 1"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Reading Data
```{r}
set.seed(123)

data <- read.csv("superconductor_dataset.csv")
```

# Required packages
```{r}
library(SmartEDA)
library(dplyr)
library(purrr)
library(ggplot2)
library(corrplot)
library(caret)
library(randomForest)
```

# EDA, first look at the dataset
```{r}
ExpData(data,type=1)
```

We have 168 numeric variables and only 1 text variable, no missing values. There are 9 columns with zero variance, we'll look at it later.

Let's look at the text variable:
`r head(data$material)`
Out of total `r nrow(data)` observations, we have `r n_distinct(data %>% select(material))` unique record in this text column.
We won't be using this variable in our analysis.

```{r}
data <- data %>% select(-material)

# Return a character vector of variable names which have 0 variance
variables_with_zero_var <- names(data)[which(map_dbl(data, var) == 0)]

summary(data[,variables_with_zero_var])

data <- data %>% select(-all_of(variables_with_zero_var))
```
Those 9 variables have zero variance, we'll exclude them

We should look into our target variable - critical_temp
```{r}
summary( data %>% select(critical_temp) )
```
We see a maximum value of critical_temp to be 185, while 3rd Quantile - 63. Let's look at the boxplot:
```{r}
ggplot(data, aes(x=critical_temp)) +
  geom_boxplot() +
  theme_minimal()
```
Seems like we have one outlier at >150. We will remove this observation
```{r}
data <- data %>% filter(critical_temp < 150)
```

```{r}
ExpNumStat(data,by ="A",Outlier=TRUE,round= 2, gp = "critical_temp") %>% 
  select(Vname, min, max, mean, median, SD, nOutliers)
```

We should look at the correlation between variables
```{r}
# Correlation

corr_simple <- function(df,sig=0.5){
  corr <- cor(df)
  #prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  corr[corr == 1] <- NA 
  #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr) 
  #select significant values  
  corr <- subset(corr, abs(Freq) > sig) 
  #sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),] 
  return(corr)
}

correlation_matrix = cor(data)
```

We have `r length(findCorrelation(correlation_matrix, cutoff = 0.99))` variables with higher than .99 correlation.
We have `r length(findCorrelation(correlation_matrix, cutoff = 0.95))` variables with higher than .95 correlation.
We have `r length(findCorrelation(correlation_matrix, cutoff = 0.9))` variables with higher than .9 correlation.

# Pre-process
## Data split
```{r}
indices <- createDataPartition(data$critical_temp, p = 0.8, list = FALSE)
train <- data[indices,]
test <- data[-indices,]
```

Splitted a dataset by 80% / 20% rule. Created a training dataset with `r nrow(train)` (`r nrow(train)/nrow(data)*100`%) observations and testing dataset with `r nrow(test)` (`r nrow(test)/nrow(data)*100`%) observations.

## Data transformation
```{r}
preProcValues <- preProcess(train, method = c("center", "scale"))
train_transformed <- predict(preProcValues, train)
test_transformed <- predict(preProcValues, test)

label_index <- which(colnames(train_transformed) == "critical_temp")
```
We also centered and scaled our data. We transformed the testing dataset based on the pre process of training dataset.

# ML algorithms
## Linear Regression
```{r}
# linear regression
train_control_cv <- trainControl(method = "cv", number = 10)
fit_lm <- train(critical_temp ~ ., data = train_transformed, method = "lm", trControl = train_control_cv)
```

We are performing linear regression model on the whole training dataset with 10 fold cross-validation.
The model:
`r print(fit_lm)`

Using fitted linear regression model on the testing dataset:
```{r}
test_lm <- predict(fit_lm, newdata = test_transformed)
print(round(postResample(pred = test_lm, obs = test_transformed$critical_temp), 3))
```
Our model, on the testing dataset, only explains ~30% of variance, while on a training set the rsquared was 72%. But the MAE was 0.36 for the training set and 0.38 for the testing dataset, it could mean an outlier influences the RMSE and rqsuared.

Let's look at the fitted vs actual plot:
```{r}
fitted_actual_lm <- data.frame(fitted = test_lm, actual = test_transformed$critical_temp)

ggplot(fitted_actual_lm,                                     
       aes(x = fitted,
           y = actual)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              size = 2) +
  theme_minimal()
```
We clearly see one value, with an actual value of ~-0.5, while the fitted value is ~-70.

Let's look at that observation before transformation:
```{r}
test[test_lm < -60,] %>% 
  tidyr::pivot_longer(cols = everything())
```

If we would exclude this observation from our testing dataset:
```{r}
# without the outlier
print(round(postResample(pred = test_lm[test_lm > -60], obs = test_transformed$critical_temp[test_lm > -60]), 3))

ggplot(fitted_actual_lm %>% filter(fitted > -60),                                     
       aes(x = fitted,
           y = actual)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              size = 2)
```
Our rsquared would be 76%.


## Linear Regression with removed correlated variables
```{r}
# Removed >.9 correlations
correlation_matrix = cor(train_transformed)
correlated_columns = findCorrelation(correlation_matrix, cutoff = 0.9)
correlated_columns = sort(correlated_columns)

colnames(train_transformed[, correlated_columns])
```
We are going to remove these columns from our training dataset, those columns have >.9 correlation


```{r}
train_transformed_no_correlated = train_transformed[,-c(correlated_columns)]
label_index_noCorr <- which(colnames(train_transformed_no_correlated) == "critical_temp")
fit_lm_no_correlated <- train(critical_temp ~ ., data = train_transformed_no_correlated, method = "lm", trControl = train_control_cv)
```

Performing Linear Regression with 10 fold cross-validation
`r print(fit_lm_no_correlated)`
We see, that our rsquared on training dataset dropped to 69%.

Testing on a test dataset
```{r}
test_lm_noCorr <- predict(fit_lm_no_correlated, newdata = test_transformed[, -c(correlated_columns, label_index)])
round(postResample(pred = test_lm_noCorr, obs = test_transformed$critical_temp), 3)
```
Rsquared on the testing dataset is 41%, we see similar results with MAE, let's look at the plot


```{r}
fitted_actual_lm_noCorr <- data.frame(fitted = test_lm_noCorr, actual = test_transformed$critical_temp)

ggplot(fitted_actual_lm_noCorr,                                     
       aes(x = fitted,
           y = actual)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              size = 2) +
  theme_minimal()
```
Same result with that one outlier, let's try without it:

```{r}
round(postResample(pred = test_lm_noCorr[test_lm_noCorr > -50], obs = test_transformed$critical_temp[test_lm_noCorr > -50]), 3)

ggplot(fitted_actual_lm_noCorr %>% filter(fitted > -50),                                     
       aes(x = fitted,
           y = actual)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              size = 2) +
  theme_minimal()
```
The rsquared increased to 74%. Slightly worse than using all variables.

# Random Forest
Out of curiosity, let's try a simple random forest model to see how it fits our data.
```{r}
fit_rf <- randomForest(x = train_transformed[, -label_index], y = train_transformed[, label_index], ntree = 150, do.trace = T)
```
```{r}
print(fit_rf)
```
Our random forest model on training dataset explains almost 93% or variance, let's try on a test dataset
```{r}
test_rf = predict(fit_rf, newdata = test_transformed[, -label_index])
round(postResample(pred = test_rf, obs = test_transformed$critical_temp), 3)
```
Rsquared on a test dataset is 93%, good results. :)



```{r}
fitted_actual_rf <- data.frame(fitted = test_rf, actual = test_transformed$critical_temp)

ggplot(fitted_actual_rf,                                     
       aes(x = fitted,
           y = actual)) +
  geom_point() +
  geom_abline(intercept = 0,
              slope = 1,
              color = "red",
              size = 2) +
  theme_minimal()
```
