---
title: "VytKra Mathematical Methods for Artificial Intelligence Lab 1"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Reading Data
```{r}
data <- read.csv("superconductor_dataset.csv")
```

# Preparation
```{r}
# List of variables names
variables_names <- c(
  "Atomic Mass",
  "First Ionization Energy",
  "Atomic Radius",
  "Density",
  "Electron Affinity",
  "Fusion Heat",
  "Thermal Conductivity",
  "Valence"
)

# List of variable keywords in a dataset
variables <- c(
  "atomic_mass",
  "fie",
  "atomic_radius",
  "Density",
  "ElectronAffinity",
  "FusionHeat",
  "ThermalConductivity",
  "Valence"
)

names(variables) <- variables_names

# Variables:
print(variables)

features_names <- c(
  "Mean",
  "Weighted mean",
  "Geometric mean",
  "Weighted geometric mean",
  "Entropy",
  "Weighted entropy",
  "Range",
  "Weighted range",
  "Standard deviation",
  "Weighted standard deviation"
)

features <- c(
  "mean",
  "wtd_mean",
  "gmean",
  "wtd_gmean",
  "entropy",
  "wtd_entropy",
  "range",
  "wtd_range",
  "std",
  "wtd_std"
)

names(features) <- features_names

# Feautres:
print(features)
```

# Data prep
```{r}
library(dplyr)
library(tidyr)
library(stringr)

# Temp list to extract parts of dataset columns
# temp <- list()
# 
# # We'll need an ID column later on
# data <- data %>% 
#   mutate(ID = seq(1:nrow(data)))
# 
# # We extract the feature name from the columns
# for(i in 1:length(variables)){
#   temp[[i]] <- data[,grepl(variables[i], colnames(data))] %>% 
#     mutate(Variable = names(variables[i]),
#            Variable_Keyword = variables[i]) %>% 
#     bind_cols(data %>% select(ID)) %>% 
#   pivot_longer(!c(Variable, Variable_Keyword, ID), names_to = "Feature_Full", values_to = "Value")
# }
# 
# # We extract the variable name and bring back to the structure of dataset
# data <- bind_rows(temp) %>% 
#   mutate(
#     Feature_Keyword = Feature_Full %>%
#       str_remove(pattern = paste0("_", Variable_Keyword)),
#     Feature = names(features[match(Feature_Keyword,  features)])
#   ) %>% 
#   select(-Variable_Keyword, -Feature_Full, -Feature_Keyword) %>% 
#   pivot_wider(names_from = c("Variable", "Feature"), values_from = "Value") %>% 
#   bind_cols(data %>% select(1,(length(features)*length(variables)+2):(ncol(data)-1))) %>% 
#   select(-ID)

```

# EDA
```{r}
library(SmartEDA)

# Overview of the data - Type = 1
ExpData(data,type=1)
# we have only 1 text variable, no missing values

n_distinct(data %>% select(material))
nrow(data)
# 15542 unique values out of 21263, we will not use this column

data <- data %>% select(-material)


library(purrr)

# Return a character vector of variable names which have 0 variance
variables_with_zero_var <- names(data)[which(map_dbl(data, var) == 0)]

summary(data[,variables_with_zero_var])
# Those columns dont hold any info, we'll exclude them

data <- data %>% select(-all_of(variables_with_zero_var))


# Target variable
summary( data %>% select(critical_temp) )


# Correlation
library(corrplot)
corr_simple <- function(df,sig=0.5){
  corr <- cor(df)
  #prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  corr[corr == 1] <- NA 
  #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr) 
  #select significant values  
  corr <- subset(corr, abs(Freq) > sig) 
  #sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),] 
  return(corr)
}

# temp <- corr_simple(data, 0.9) %>% 
#   separate(Var1, c("Var1-Variable", "Var1-Feature"), sep = "_", extra = "merge", remove = FALSE) %>% 
#   separate(Var2, c("Var2-Variable", "Var2-Feature"), sep = "_", extra = "merge", remove = FALSE)
# 
# 
# different_variables <- temp %>% 
#   filter(`Var1-Variable` != `Var2-Variable`)
# 
# same_variables <- temp %>% 
#   filter(`Var1-Variable` == `Var2-Variable`)

```

# Data split
```{r}
set.seed(123)
library(caret)
indices <- createDataPartition(data$critical_temp, p = 0.8, list = FALSE)
train <- data[indices,]
test <- data[-indices,]

preProcValues <- preProcess(train, method = c("center", "scale"))
train_transformed <- predict(preProcValues, train)
test_transformed <- predict(preProcValues, test)

label_index <- which(colnames(train_transformed) == "critical_temp")
```


# Task 2
```{r}
library(parallel)
library(doParallel)

cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)

# linear regression
train_control_cv <- trainControl(method = "cv", number = 10, allowParallel = TRUE)
fit_lm <- train(critical_temp ~ ., data = train_transformed, method = "lm", trControl = train_control_cv)
print(fit_lm)

test_lm <- predict(fit_lm, newdata = test_transformed)
round(postResample(pred = test_lm, obs = test_transformed$critical_temp), 3)


# random forest
fit_rf <- train(critical_temp ~ ., data = train_transformed, method = "rf") # pridet cv
print(fit_rf)
test_rf <- predict(fit_rf, newdata = test_transformed)
round(postResample(pred = test_rf, obs = test_transformed$critical_temp), 3)

# glmnet
fit_glmnet <- train(critical_temp ~ ., data = train_transformed, method = "glmnet") # pridet cv
print(fit_glmnet)
test_glmnet <- predict(fit_glmnet, newdata = test_transformed)
round(postResample(pred = test_glmnet, obs = test_transformed$critical_temp), 3)

arrange(fit_glmnet$results, RMSE) %>% head
fit_glmnet$bestTune


# gradient boosting
# We use expand grid to create a table of all possible combinations
tg <- expand.grid(shrinkage = seq(0.1, 1, by = 0.2), 
                  interaction.depth = c(1, 3, 7, 10),
                  n.minobsinnode = c(2, 5, 10),
                  n.trees = c(100, 300, 500, 1000))
fit_gbm <- train(critical_temp ~ ., data = train_transformed, method = "gbm", tuneGrid =tg, verbose = FALSE) # cv pridet
print(fit_gbm)
test_gbm <- predict(fit_gbm, newdata = test_transformed)
round(postResample(pred = test_gbm, obs = test_transformed$critical_temp), 3)

arrange(fit_gbm$results, RMSE) %>% head
fit_gbm$bestTune


# Removed >.95 correlations
correlation_matrix = cor(train_transformed)
correlated_columns = findCorrelation(correlation_matrix, cutoff = 0.95)
correlated_columns = sort(correlated_columns)
train_transformed_no_correlated_95 = train_transformed[,-c(correlated_columns)]

fit_lm_no_correlated_95 <- train(critical_temp ~ ., data = train_transformed_no_correlated_95, method = "lm", trControl = train_control_cv)
print(fit_lm_no_correlated_95)
test_lm_noCorr <- predict(fit_lm_no_correlated_95, newdata = test_transformed)
round(postResample(pred = test_lm_noCorr, obs = test_transformed$critical_temp), 3)

stopCluster(cluster)


mods <- resamples(list(lm = fit_lm, rf = fit_rf , glmnet = fit_glmen, gbm = fit_gbm, lm_noCorr = fit_lm_no_correlated_95))
summary(mods)
#compare_models(glmnet_model, gbm_model)





# xgboost
tune_grid <- expand.grid(nrounds=c(100,200,300,400),
                        max_depth = c(3:7),
                        eta = c(0.05, 1),
                        gamma = c(0.01),
                        colsample_bytree = c(0.75),
                        subsample = c(0.50),
                        min_child_weight = c(0))

fit_xgboost <- train(critical_temp ~., data = train_transformed, method = "xgbTree",
                trControl = train_control_cv,
                tuneGrid = tune_grid,
                tuneLength = 10)




```

```{r}
library(randomForest)
fit_rf <- randomForest(x = train_transformed[, -label_index], y = train_transformed[, label_index])
```

# xgboost
```{r}
# https://www.r-bloggers.com/2020/11/r-xgboost-regression/
library(xgboost)
library(DiagrammeR)
set.seed(123)
NewData_xgboost <- as.matrix(data)
indices <- sample(1:nrow(NewData_xgboost), size = 0.75 * nrow(NewData_xgboost))
train <- NewData_xgboost[indices,]
test <- NewData_xgboost[-indices,]

label_index <- which(colnames(NewData_xgboost) == "critical_temp")

m1_xgb <-
  xgboost(
    data = train[, -label_index],
    label = train[, label_index],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    max_depth = 6,
    eta = .25
  )

pred_xgb <- predict(m1_xgb, test[, -label_index])

yhat <- pred_xgb
y <- test[, label_index]
postResample(yhat, y)


r <- y - yhat
plot(r, ylab = "residuals", main = title)


plot(y,
     yhat,
     xlab = "actual",
     ylab = "predicted",
     main = title)
abline(lm(yhat ~ y))


#plot first 3 trees of model
xgb.plot.tree(model = m1_xgb, trees = 0:2)

importance_matrix <- xgb.importance(model = m1_xgb)
xgb.plot.importance(importance_matrix, xlab = "Feature Importance")

head(importance_matrix)



#grid search
#create hyperparameter grid
hyper_grid <- expand.grid(max_depth = seq(3, 6, 1),
                          eta = seq(.2, .35, .01))
xgb_train_rmse <- NULL
xgb_test_rmse <- NULL

for (j in 1:nrow(hyper_grid)) {
  set.seed(123)
  m_xgb_untuned <- xgb.cv(
    data = train[, 2:34],
    label = train[, 1],
    nrounds = 1000,
    objective = "reg:squarederror",
    early_stopping_rounds = 3,
    nfold = 5,
    max_depth = hyper_grid$max_depth[j],
    eta = hyper_grid$eta[j]
  )
  
  xgb_train_rmse[j] <- m_xgb_untuned$evaluation_log$train_rmse_mean[m_xgb_untuned$best_iteration]
  xgb_test_rmse[j] <- m_xgb_untuned$evaluation_log$test_rmse_mean[m_xgb_untuned$best_iteration]
  
  cat(j, "\n")
}

#ideal hyperparamters
hyper_grid[which.min(xgb_test_rmse), ]












```