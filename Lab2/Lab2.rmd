---
title: "Mathematical Methods for Artificial Intelligence Lab 2"
author: "Vytautas Kraujalis"
date: '2021-10-03'
output: 
  html_document:
    toc: true 
    toc_depth: 2
    number_sections: true
    theme: united
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Reading Data
```{r}
set.seed(123)

data <- read.csv("water_potability.csv")
```

# Required packages
```{r}
library(SmartEDA)
library(dplyr)
library(ggplot2)
library(mice)
library(VIM)
library(glmnet)
library(caret)
library(ROCR)

# Function for binary class accuracy from confusion matrix
classAcc <- function(confusionMatrix) {
  class0 <- round(confusionMatrix$table[1, 1] / sum(confusionMatrix$table[, 1]) * 100, 1)
  class1 <- round(confusionMatrix$table[2, 2] / sum(confusionMatrix$table[, 2]) * 100, 1)
  acc <- c(class0, class1)
  names(acc) <- c("Acc: 0", "Acc: 1")
  return(acc)
}
```

# EDA, first look at the dataset
```{r}
ExpData(data, type = 1)
```

We can see, that we have 10 numerical variables and 3276 observations, we can also notice, that 3 variables have missing variables.

```{r}
summary(data)
```

Variables "ph", "Sulfate" and "Trihalomethanes" have missing values. 


Let's look how our missing values are distributed:



```{r}
aggr(data, col = c("navyblue", "red"), numbers = TRUE, sortVars = TRUE, labels = names(data), cex.axis = .7, gap = 3, ylab = c("Histogram of missing data", "Pattern"))
```

Out of `r nrow(data)` observations we have 61% of observations without any missing value. 19% of observations have only one missing variable. Variable "ph" contains almost 15% of missing values, while "Sulfate" has almost 24% of missing values, this may cause problems. "Trihalomethanes" has less than <5% of missing values.

We are going to use "mice" package for missing values imputation.
```{r}
imp <- mice(data)

summary(imp)
```

```{r}
data_noMissing <- complete(imp, 1)

xyplot(imp, Potability ~ ph + Sulfate + Trihalomethanes, pch = 18, cex = 1)

densityplot(imp)
```

The scatter plot of imputed data (red) and observed values (blue) shows that we did not produced any outliers. Density plots also shows no bad variation of imputed data.

# Pre-process
## Data split
```{r}
indices <- createDataPartition(data_noMissing$Potability, p = 0.8, list = FALSE)
train <- data_noMissing[indices, ]
test <- data_noMissing[-indices, ]

label_index <- which(colnames(train) == "Potability")
```

Splitted a dataset by 80% / 20% rule. Created a training dataset with `r nrow(train)` (`r nrow(train)/nrow(data)*100`%) observations and testing dataset with `r nrow(test)` (`r nrow(test)/nrow(data)*100`%) observations.

## Data transformation
```{r}
preProcValues <- preProcess(train[, -label_index], method = c("center", "scale"))
train[, -label_index] <- predict(preProcValues, train[, -label_index])
test[, -label_index] <- predict(preProcValues, test[, -label_index])
```

We also centered and scaled our data. We transformed the testing dataset based on the pre process of training dataset.

# Logistic Regression
## Simple Logistic Regression
```{r}
fit_glm_cv <- cv.glmnet(as.matrix(train[, -label_index]), train[, label_index], family = "binomial", type.measure = "auc", keep = T, nfolds = 10)
plot(fit_glm_cv)
grid()
```

Best lambda:
```{r}
fit_glm_cv$lambda.min
```


At first glance, our model on the training set does not perform well. Let's take the best result and run a logistic regression again

Using fitted logistic regression model on the testing dataset:
```{r}
fit_glm <- glmnet(as.matrix(train[, -label_index]), train[, label_index], family = "binomial", lambda = fit_glm_cv$lambda.min)
test_glm <- predict(fit_glm, as.matrix(test[, -label_index]), type = "class")
confusionMatrix <- confusionMatrix(as.factor(test_glm), as.factor(test$Potability), mode = "everything")
print(confusionMatrix)
print(classAcc(confusionMatrix))
```

The overall accuracy is 63%, but the No Information Rate is 0.63. We can see from the confusion table and from the NIR, that our model poorly predicts one class. Class "1" has 0% accuracy 

Let's check the ROC curves:
```{r}
prob_glm <- predict(fit_glm, as.matrix(test[, -label_index]), type = "response", s = fit_glm$lambda)
pred_glm <- prediction(prob_glm, test[, label_index])
perf_glm <- performance(pred_glm, measure = "tpr", x.measure = "fpr")

# area under the curve
auc <- attr(performance(pred_glm, "auc"), "y.values")[[1]]
auc <- round(auc, digits = 3)

roc_glm <- data.frame(fpr = attr(perf_glm, "x.values")[[1]], tpr = attr(perf_glm, "y.values")[[1]])

ggplot(roc_glm, aes(x = fpr, ymin = 0, ymax = tpr)) +
  geom_ribbon(alpha = 0.2) +
  geom_line(aes(y = tpr)) +
  ggtitle(paste0("ROC Curve, AUC = ", auc)) +
  geom_abline(slope = 1, color = "red", size = 1) +
  theme_minimal()
```

The AUC is 0.465, so our model is even worse than a random guess.

## Logistic Regression with interactions

Let's try to add interactions between variables to see if that could lead to a better prediction.
```{r}
formula <- as.formula(" ~ .^2")

# We add interactions of our primary variables
train_int <- model.matrix(formula, data = train[, -label_index])
train_int <- train_int[, 2:ncol(train_int)] %>%
  bind_cols(Potability = train$Potability) %>%
  as.matrix()
train_int_label_index <- which(colnames(train_int) == "Potability")

test_int <- model.matrix(formula, data = test[, -label_index])
test_int <- test_int[, 2:ncol(test_int)] %>%
  bind_cols(Potability = test$Potability) %>%
  as.matrix()

fit_glm_cv_int <- cv.glmnet(train_int[, -train_int_label_index], train_int[, train_int_label_index], family = "binomial", type.measure = "auc", keep = T, nfolds = 10)
plot(fit_glm_cv_int)
grid()
```

Best lambda:
```{r}
fit_glm_cv_int$lambda.min
```

The addition of interactions seems to give a better result, let's try the fitted model on a test dataset:

```{r}
fit_glm_int <- glmnet(train_int[, -train_int_label_index], train_int[, train_int_label_index], family = "binomial", lambda = fit_glm_cv_int$lambda.min)
test_glm_int <- predict(fit_glm_int, test_int[, -train_int_label_index], type = "class")
confusionMatrix <- confusionMatrix(as.factor(test_glm_int), as.factor(test_int[, train_int_label_index]), mode = "everything")
print(confusionMatrix)
print(classAcc(confusionMatrix))
```

The overall accuracy is 68%, but now, the addition of interactions between variables led to 18% accuracy of the "1" class, while the accuracy for class "0" only dropped to 97%.

Let's check the ROC curves:
```{r}
prob_glm_int <- predict(fit_glm_int, test_int[, -train_int_label_index], type = "response", s = fit_glm_int$lambda)
pred_glm_int <- prediction(prob_glm_int, test_int[, train_int_label_index])
perf_glm_int <- performance(pred_glm_int, measure = "tpr", x.measure = "fpr")

# area under the curve
auc <- attr(performance(pred_glm_int, "auc"), "y.values")[[1]]
auc <- round(auc, digits = 3)

roc_glm_int <- data.frame(fpr = attr(perf_glm_int, "x.values")[[1]], tpr = attr(perf_glm_int, "y.values")[[1]])

ggplot(roc_glm_int, aes(x = fpr, ymin = 0, ymax = tpr)) +
  geom_ribbon(alpha = 0.2) +
  geom_line(aes(y = tpr)) +
  ggtitle(paste0("ROC Curve, AUC = ", auc)) +
  geom_abline(slope = 1, color = "red", size = 1) +
  theme_minimal()
```

This time, the AUC is 0.692 and our model looks better than a random guess.

## Logistic Regression with interactions and 2nd order polynomials

Interactions seemed to help in prediction, now let's try to add 2nd order polynomials of variables.
```{r}
formula <- as.formula(paste(" ~ .^2 + ", paste("poly(", colnames(train[, -label_index]), ",2, raw=TRUE)[, 2]", collapse = " + ")))

# We add interactions of our primary variables
train_int_pol2 <- model.matrix(formula, data = train[, -label_index])
train_int_pol2 <- train_int_pol2[, 2:ncol(train_int_pol2)] %>%
  bind_cols(Potability = train$Potability) %>%
  as.matrix()
train_int_pol2_label_index <- which(colnames(train_int_pol2) == "Potability")

test_int_pol2 <- model.matrix(formula, data = test[, -label_index])
test_int_pol2 <- test_int_pol2[, 2:ncol(test_int_pol2)] %>%
  bind_cols(Potability = test$Potability) %>%
  as.matrix()

fit_glm_cv_int_pol2 <- cv.glmnet(train_int_pol2[, -train_int_pol2_label_index], train_int_pol2[, train_int_pol2_label_index], family = "binomial", type.measure = "auc", keep = T, nfolds = 10)
plot(fit_glm_cv_int_pol2)
grid()
```

Best lambda:
```{r}
fit_glm_cv_int_pol2$lambda.min
```

The addition of interactions and 2nd order polynomials seems to give a slightly better result than just with interactions, let's try the fitted model on a test dataset:

```{r}
fit_glm_int_pol2 <- glmnet(train_int_pol2[, -train_int_pol2_label_index], train_int_pol2[, train_int_pol2_label_index], family = "binomial", lambda = fit_glm_cv_int_pol2$lambda.min)
test_glm_int_pol2 <- predict(fit_glm_int_pol2, test_int_pol2[, -train_int_pol2_label_index], type = "class")
confusionMatrix <- confusionMatrix(as.factor(test_glm_int_pol2), as.factor(test_int_pol2[, train_int_pol2_label_index]), mode = "everything")
print(confusionMatrix)
print(classAcc(confusionMatrix))
```

The overall accuracy is 69%, but now, the addition of interactions between variables and 2nd order polynomials led to 21% accuracy of the "1" class, while the accuracy for class "0" only dropped to 96%.

Let's check the ROC curves:
```{r}
prob_glm_int_pol2 <- predict(fit_glm_int_pol2, test_int_pol2[, -train_int_pol2_label_index], type = "response", s = fit_glm_int_pol2$lambda)
pred_glm_int_pol2 <- prediction(prob_glm_int_pol2, test_int_pol2[, train_int_pol2_label_index])
perf_glm_int_pol2 <- performance(pred_glm_int_pol2, measure = "tpr", x.measure = "fpr")

# area under the curve
auc <- attr(performance(pred_glm_int_pol2, "auc"), "y.values")[[1]]
auc <- round(auc, digits = 3)

roc_glm_int_pol2 <- data.frame(fpr = attr(perf_glm_int_pol2, "x.values")[[1]], tpr = attr(perf_glm_int_pol2, "y.values")[[1]])

ggplot(roc_glm_int_pol2, aes(x = fpr, ymin = 0, ymax = tpr)) +
  geom_ribbon(alpha = 0.2) +
  geom_line(aes(y = tpr)) +
  ggtitle(paste0("ROC Curve, AUC = ", auc)) +
  geom_abline(slope = 1, color = "red", size = 1) +
  theme_minimal()
```

This time, the AUC increased to 0.7. Overall, just a small increase after the interactions.

# Conclusion

A logistic regression with interactions between variables and 2nd order polynomials seemed to give the best prediction. Addition of 2nd order polynomial just slightly increased the AUC and accuracy, so there's no need to try the next order.

Best lambda, obtained from the cross-validation, is `r round(fit_glm_cv_int_pol2$lambda.min, 3)`.
The overall accuracy on a test dataset is `r round(confusionMatrix$overall[1] , 2)*100`%, while the accurcy for "0" class is `r round(classAcc(confusionMatrix)[1], 2)`% and for class "1" is `r round(classAcc(confusionMatrix)[2], 2)`%. 

The AUC is `r auc`


