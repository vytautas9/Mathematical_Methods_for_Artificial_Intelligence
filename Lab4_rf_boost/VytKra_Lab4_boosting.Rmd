---
title: "Mathematical Methods for Artificial Intelligence Lab 4 - Boosting Alg."
author: "Vytautas Kraujalis"
date: '2021-12-23'
output: 
  word_document:
    toc: true 
    toc_depth: 3
    number_sections: true
    highlight: tango
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Gradient Boosting
## Required packages
```{r}
library(data.table)
library(dplyr)
library(tictoc)
library(janitor)
library(caret)
library(xgboost)

# Function for 4 class accuracy from confusion matrix
classAcc <- function(confusionMatrix) {
  class1 <- round(confusionMatrix$table[1, 1] / sum(confusionMatrix$table[, 1]) * 100, 1)
  class2 <- round(confusionMatrix$table[2, 2] / sum(confusionMatrix$table[, 2]) * 100, 1)
  class3 <- round(confusionMatrix$table[3, 3] / sum(confusionMatrix$table[, 3]) * 100, 1)
  class4 <- round(confusionMatrix$table[4, 4] / sum(confusionMatrix$table[, 4]) * 100, 1)
  acc <- c(class1, class2, class3, class4 )
  names(acc) <- colnames(confusionMatrix$table)
  return(acc)
}

# remove scientific notation
options(scipen = 100)
```


## Reading Data
```{r}
set.seed(123)

data_original <- fread("activity.csv")
data_names <- read.table("names.txt") %>% 
   rename(column_names = V1)
```

## Data Preparation
```{r}
data <- data_original
colnames(data) <- data_names$column_names

data <- data %>% 
   clean_names() %>% 
   mutate_if(is.character, as.factor)
data <- data %>% 
   select(-nearZeroVar(data))
```

```{r}
# Correlation

corr_simple <- function(df,sig=0.5){
  corr <- cor(df)
  #prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  corr[corr == 1] <- NA 
  #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr) 
  #select significant values  
  corr <- subset(corr, abs(Freq) > sig) 
  #sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),] 
  return(corr)
}

correlation_matrix = cor(data %>% select(-activity, -subject_index))
```

```{r}
data <- data %>% 
   select(-findCorrelation(correlation_matrix, cutoff = 0.99)) %>% 
   mutate(subject_index = as.factor(subject_index))
```

## Fitting Xgboost model

### Tuning learning rate
We'll look for an optimal learning rate which we will use to find other optimal parameters.
```{r}
tune_grid <- expand.grid(
  nrounds = seq(from = 200, to = 1000, by = 50),
  eta = c(0.025, 0.05, 0.1, 0.3),
  max_depth = c(2, 3, 4, 5, 6),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

folds = groupKFold(data$subject_index, k = 3)
tune_control <- trainControl(
  index = folds,
  method = "cv",
  number = 3,
  verboseIter = FALSE#, allowParallel = TRUE
  )

tic()
xgb_tune <- train(
  activity ~ ., data = data %>% select(-subject_index),
  trControl = tune_control,
  tuneGrid = tune_grid,
  method = "xgbTree",
  verbose = TRUE
)
toc()
```

```{r}
# helper function for the plots
tuneplot <- function(x, probs = .10) {
  ggplot(x) +
    coord_cartesian(ylim = c(quantile(x$results$Accuracy, probs = probs), max(x$results$Accuracy))) +
    theme_bw()
}

tuneplot(xgb_tune)

xgb_tune$bestTune %>% 
   mutate_all(round, digits = 5) %>% 
   t()
```

Seems like the best accuracy is reached using `r xgb_tune$bestTune$eta` learning rate with `r xgb_tune$bestTune$nrounds` number of trees and with max tree depth equal to `r xgb_tune$bestTune$max_depth`. We will use this learning rate in our further tuning analysis.

### Tuning max tree depth and minimum child weight
```{r}

tune_grid2 <- expand.grid(
  nrounds = seq(from = 50, to = 1000, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = if(xgb_tune$bestTune$max_depth == 2){
     seq(xgb_tune$bestTune$max_depth, 4, 1)
  }else{
     seq(xgb_tune$bestTune$max_depth - 1, xgb_tune$bestTune$max_depth + 1, 1)
  },
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = c(1, 2, 3, 4),
  subsample = 1
)

tic()
xgb_tune2 <- train(
  activity ~ ., data = data %>% select(-subject_index),
  trControl = tune_control,
  tuneGrid = tune_grid2,
  method = "xgbTree",
  verbose = TRUE
)
toc()
```

```{r}
tuneplot(xgb_tune2)

xgb_tune2$bestTune %>% 
   t()
```

Looks like a better accuracy can be obtained using `r xgb_tune2$bestTune$min_child_weight` minimal child weight and with `r xgb_tune2$bestTune$max_depth` max depth, same as before.


### Tuning row and column sampling
```{r}
tune_grid3 <- expand.grid(
  nrounds = seq(from = 50, to = 1000, by = 50),
  eta = xgb_tune$bestTune$eta,
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = 0,
  colsample_bytree = c(0.4, 0.6, 0.8, 1.0),
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = c(0.5, 0.75, 1.0)
)

tic()
xgb_tune3 <- train(
  activity ~ ., data = data %>% select(-subject_index),
  trControl = tune_control,
  tuneGrid = tune_grid3,
  method = "xgbTree",
  verbose = TRUE
)
toc()
```

```{r}
tuneplot(xgb_tune3)

xgb_tune3$bestTune %>% 
   t()
```

Best accuracy is reached using `r xgb_tune3$bestTune$colsample_bytree` column sample and `r xgb_tune3$bestTune$subsample` row sample.

### Final tune for number of trees and learning rate with other optimal parameters
```{r}
tune_grid4 <- expand.grid(
  nrounds = seq(from = 100, to = 10000, by = 100),
  eta = c(0.001, 0.005, 0.01, 0.015, 0.02, 0.025),
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = 0,
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)

tic()
xgb_tune4 <- train(
  activity ~ ., data = data %>% select(-subject_index),
  trControl = tune_control,
  tuneGrid = tune_grid4,
  method = "xgbTree",
  verbose = TRUE
)
toc()
```

```{r}
tuneplot(xgb_tune4)
```

Final parameters:
```{r}
xgb_tune4$bestTune %>% 
   t()
```


```{r}
final_grid <- expand.grid(
  nrounds = xgb_tune4$bestTune$nrounds,
  eta = xgb_tune4$bestTune$eta,
  max_depth = xgb_tune4$bestTune$max_depth,
  gamma = 0,
  colsample_bytree = xgb_tune4$bestTune$colsample_bytree,
  min_child_weight = xgb_tune4$bestTune$min_child_weight,
  subsample = xgb_tune4$bestTune$subsample
)
```

### Fitting model with final optimal parameters
```{r}
folds = groupKFold(data$subject_index, k = 3)
train_control <- trainControl(
  index = folds,
  method = "cv",
  number = 3,
  verboseIter = FALSE,
  classProbs = TRUE,
  savePredictions='all'
  )

tic()
xgb_model <- train(
  activity ~ ., data = data %>% select(-subject_index),
  trControl = train_control,
  tuneGrid = final_grid,
  method = "xgbTree",
  verbose = TRUE
)
toc()
```

```{r}
xgb_final <- xgb_model$finalModel
```

```{r}
data_pred <- predict(xgb_final, newdata = data %>% select(-subject_index, -activity) %>% data.matrix())
data_prediction <- matrix(data_pred, nrow = 4,
                          ncol = length(data_pred)/4) %>%
  t() %>%
  data.frame() %>%
  mutate(label = as.numeric(data$activity),
         max_prob = max.col(., "last"))

confusion_matrix <- confusionMatrix(factor(data_prediction$max_prob), factor(data_prediction$label))
confusion_matrix
```

Final model has overall accuracy of `r confusion_matrix$overall["Accuracy"] * 100`%, while for each class accuracies are:
```{r}
classAccuracies <- confusion_matrix$byClass[,"Sensitivity"]*100
names(classAccuracies) <- levels(data$activity)
classAccuracies
```

The model predicts physical class almost perfectly but struggles with other 3 classes.

```{r}
importance <- xgb.importance(model = xgb_final)

importance %>% 
   xgb.ggplot.importance(top_n = 25, measure = NULL, rel_to_first = F)
```

According to feature importance and top 25 variables, there are 2 variables that stood up: one from ECG and one from TEB type of feature. Those 2 features are the most important according to the xgboost method.

## References
https://www.kaggle.com/pelkoja/visual-xgboost-tuning-with-caret
https://www.r-bloggers.com/2019/10/explaining-predictions-boosted-trees-post-hoc-analysis-xgboost/