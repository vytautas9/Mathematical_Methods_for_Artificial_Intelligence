---
title: "Mathematical Methods for Artificial Intelligence Lab 4 - Random Forest"
author: "Vytautas Kraujalis"
date: '2021-12-18'
output: 
  word_document:
    toc: true 
    toc_depth: 3
    number_sections: true
    highlight: tango
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Random Forest
## Required packages
```{r}
library(data.table)
library(SmartEDA)
library(dplyr)
library(ggplot2)
library(caret)
library(randomForest)
library(janitor)
library(tictoc)
library(tidyr)
library(tibble)
library(ggrepel)

# Function for 4 class accuracy from confusion matrix
classAcc <- function(confusionMatrix) {
  class1 <- round(confusionMatrix[1, 1] / sum(confusionMatrix[, 1]) * 100, 1)
  class2 <- round(confusionMatrix[2, 2] / sum(confusionMatrix[, 2]) * 100, 1)
  class3 <- round(confusionMatrix[3, 3] / sum(confusionMatrix[, 3]) * 100, 1)
  class4 <- round(confusionMatrix[4, 4] / sum(confusionMatrix[, 4]) * 100, 1)
  acc <- c(class1, class2, class3, class4 )
  names(acc) <- colnames(confusionMatrix)
  return(acc)
}
```

## Parallel processing
```{r}
library(parallel) 
no_cores <- detectCores() - 1
library(doParallel)
cl <- makePSOCKcluster(no_cores)
registerDoParallel(cl)
```

## Reading Data
```{r}
set.seed(123)

data_original <- fread("activity.csv")
data_names <- read.table("names.txt") %>% 
   rename(column_names = V1)
```

## Data Preparation
```{r}
length(data_names$column_names)

n_distinct(data_names$column_names)
```

There are 535 provided column names, but only 417 are distinct, it means we have some duplicated names, we need to make them unique. To do that, for duplicated names we'll add a unique ID at the end:

```{r}
data <- data_original
colnames(data) <- data_names$column_names

data <- data %>% 
   clean_names()

n_distinct(colnames(data))
```

## EDA, first look at the dataset
```{r}
ExpData(data,type = 1)
```

We have a dataset of 4480 observations with 535 variables, only 1 variable has text format. All variables have no missing values. We can see that there are 4 variables with zero variance, we'll remove those later.

Let's look at the response variable:

```{r}
data %>% 
   group_by(activity) %>% 
   summarise(n = n()) %>% 
   mutate(n_prop = round(n / sum(n) * 100, 2))
```

We have perfectly balanced response variable with 4 classes.

We'll change the response variable to factor type.

```{r}
data <- data %>% 
   mutate_if(is.character, as.factor)
```

```{r}
data %>% 
   select(nearZeroVar(data)) %>% 
   summary()
```

As mentioned previously, we have 4 variables with zero variance, we will remove those columns.

```{r}
data <- data %>% 
   select(-nearZeroVar(data))
```

We should look at the correlation between variables
```{r}
# Correlation

corr_simple <- function(df,sig=0.5){
  corr <- cor(df)
  #prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  corr[corr == 1] <- NA 
  #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr) 
  #select significant values  
  corr <- subset(corr, abs(Freq) > sig) 
  #sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),] 
  return(corr)
}

correlation_matrix = cor(data %>% select(-activity, -subject_index))

length(findCorrelation(correlation_matrix, cutoff = 0.99))
length(findCorrelation(correlation_matrix, cutoff = 0.95))
length(findCorrelation(correlation_matrix, cutoff = 0.9))
```

We have 309 variables with correlation greater than 0.99, we will remove those variables.

```{r}
data <- data %>% 
   select(-findCorrelation(correlation_matrix, cutoff = 0.99))
```

We'll convert subject index column to factor type:

```{r}
data %>% 
   select(subject_index) %>% 
   table()

data <- data %>% 
   mutate(subject_index = as.factor(subject_index))
```

Let's look how obersvations are distributed across subjects and activity types:
```{r}
data %>% 
   group_by(subject_index, activity) %>% 
   summarise(n = n()) %>% 
   pivot_wider(names_from = activity, values_from = n)
```

Each person has 28 observations of each activity. We are not going to use Out-of-Bag score for tuning parameters as this would be misleading. Same person with the same activity could be splited into different sets and the result based on OOB could be misleading. We are not going to have this problem with Cross Validation, as we can specify folds to be grouped according to subjects.

## Fitting Random Forest Model

### Tune with OOB
Let's look at the optimal mtry value based on OOB and number of trees = 500
```{r}
tic()
png(file = "Tune_oob.png", width = 1200, height = 850)
rf_tune <- tuneRF(data %>% 
                     select(-subject_index, -activity),
                  data$activity, 
                  mtryStart = 2, 
                  ntreeTry = 500,
                  stepFactor = 3,
                  improve = 0.001,
                  trace = TRUE,
                  plot = TRUE)
dev.off()
toc()
```

Based on OOB the optimal mtry was found to be `r rf_tune[which(rf_tune[,2] == min(rf_tune[,2])),1]`. We are not going to use this value as mentioned previously.

### Random search
```{r}
folds = groupKFold(data$subject_index, k = 10)
fitControl <- trainControl(## 10-fold CV
  index = folds,
  method = "cv",
  number = 10,
  classProbs = TRUE,
  savePredictions='all',
  verboseIter = TRUE,
  allowParallel = TRUE,
  search = "random")

tic()
rf_random <- train(
   activity ~ ., 
   data = data %>% select(-subject_index), 
   method = "rf", 
   metric = "Accuracy", 
   tuneLength = 10, 
   trControl = fitControl)
toc()

print(rf_random)

png(file = "Tune_randomSearch.png", width = 1200, height = 850)
plot(rf_random)
dev.off()
```

Based on random search, the optimal mtry was found to be `r rf_random$bestTune$mtry`. We are going to also use a grid search, to look if an optimal mtry could be smalle and closer to sqrt(m) value.

### Grid Search
```{r}
folds = groupKFold(data$subject_index, k = 10)
fitControl <- trainControl(## 10-fold CV
  index = folds,
  method = "cv",
  number = 10,
  classProbs = TRUE,
  savePredictions='all',
  verboseIter = TRUE,
  allowParallel = TRUE,
  search = "grid")

rf_grid = expand.grid(.mtry = c(seq(2,15,2), seq(rf_random$bestTune$mtry - 4, rf_random$bestTune$mtry + 4, 2)))

tic()
rf_fit_grid_cv <- train(
   activity ~ ., 
   data = data %>% select(-subject_index), 
   method = "rf", 
   metric = "Accuracy",
   tuneGrid = rf_grid,
   trControl = fitControl)
toc()

print(rf_fit_grid_cv)

png(file = "Tune_gridSearch.png", width = 1200, height = 850)
plot(rf_fit_grid_cv)
dev.off()
```

Using grid search we found an optimal value of mtry to be `r rf_fit_grid_cv$bestTune$mtry`. We are going to use this value in our further analysis.

### Tune number of trees
```{r}
folds = groupKFold(data$subject_index, k = 10)
fitControl <- trainControl(## 10-fold CV
  index = folds,
  method = "cv",
  number = 10,
  classProbs = TRUE,
  savePredictions='all',
  verboseIter = TRUE,
  allowParallel = TRUE,
  search = "grid")

rf_grid = expand.grid(.mtry = rf_fit_grid_cv$bestTune$mtry)

modellist <- list()
for (ntree in c(350, 500, 750, 1000, 1500)) {
	fit <- train(
   activity ~ ., 
   data = data %>% select(-subject_index), 
   method = "rf", 
   metric = "Accuracy",
   tuneGrid = rf_grid,
   trControl = fitControl,
   ntree = ntree)
	key <- toString(ntree)
	modellist[[key]] <- fit
}
# compare results
results <- resamples(modellist)
summary(results)

png(file = "Tune_ntree.png", width = 1200, height = 850)
dotplot(results)
dev.off()

mtry_optimal <- rf_fit_grid_cv$bestTune$mtry

ntree_optimal <- as.integer(names(which(summary(results)$statistics$Accuracy[,"Mean"] == max(summary(results)$statistics$Accuracy[,"Mean"]))))
```

Optimal number of trees were found to be `r ntree_optimal`.

### Optimal Random Forest model
Fit a random forest model with best parameters (CV)
```{r}
folds = groupKFold(data$subject_index, k = 10)
fitControl <- trainControl(## 10-fold CV
  index = folds,
  method = "cv",
  number = 10,
  classProbs = TRUE,
  savePredictions='all',
  verboseIter = TRUE,
  allowParallel = TRUE)

tic()
rf_fit_cv <- train(
   activity ~ ., 
   data = data %>% select(-subject_index), 
   method = "rf", 
   tuneGrid = expand.grid(.mtry = mtry_optimal),
   ntree = ntree_optimal,
   trControl = fitControl,
   importance = TRUE)
toc()

```

```{r}
rf_final <- rf_fit_cv$finalModel
```


```{r}
print(rf_final)

confusion_matrix <- rf_final$confusion[,1:4]
classAcc(confusion_matrix)
```

Final model has accuracy of `r round(sum(diag(confusion_matrix)) / nrow(data) * 100, 2)`%. 

```{r}
rf_margin <- margin(rf_final, data$activity)
df = data.frame(margin = as.numeric(rf_margin), 
                label = names(rf_margin), 
                index =  1:length(rf_margin))
df %>% 
   ggplot(aes(x = index,y = margin,col = label)) + 
   geom_point() +
   theme(text = element_text(size = 14))

ggsave(filename = "Margin_ScatterPlot.png", width = 14, height = 7, units = "in", bg = "white")

df %>% 
   ggplot(aes(x = margin, fill = label)) + 
   geom_histogram() + 
   facet_wrap(~label) +
   theme(text = element_text(size = 14))

ggsave(filename = "Margin_BarPlot.png", width = 14, height = 7, units = "in", bg = "white")
```

We clearly see, that only a small number of observations were misclassified (margin < 0). We also clearly see that majority of Neutral and Physical activity observations were classified correctly with almost perfect voting score for the correct class. One could also identify, that for classes Emotional and Mental, most of the correct guesses were made with ~0.5 - ~0.75 majority votes. To increase the model effectiveness, one should find more features to distinguish Emotional and Mental classes.

Let's explore model performance for each subject:
```{r}
# get a table of actual and predicted values for each subject
df <- data.frame(
   subject_id = data$subject_index,
   activity = rf_final$y,
   activity_prediction = rf_final$predicted
)

# list of tables by each subject
df_subject <- split(df, df$subject_id)

# list of confusion matrices by each subject
df_subject <- lapply(df_subject, function(x){confusionMatrix(x$activity_prediction, x$activity)$table})

# list of class and overall accuracies by each subject
df_subject <-lapply(df_subject, function(x){
   AccClasses = classAcc(x)
   AccOverall = round(sum(diag(x)) / sum(colSums(x)) * 100, 2)
   acc = c(AccClasses, AccOverall)
   names(acc) = c(names(AccClasses), "OVERALL")
   return(acc)
})

df_colnames <- names(df_subject[[1]])

# a table of class and overall accuracies by each subject
df_subject <- do.call(rbind.data.frame, df_subject) %>% 
   rownames_to_column("subject_id") %>% 
   mutate(subject_id = as.numeric(subject_id))
colnames(df_subject) <- c("subject_id", df_colnames)

# Plot of overall
df_subject %>%
  arrange(OVERALL) %>%
  mutate(subject_id = factor(subject_id, levels = subject_id)) %>% 
  ggplot( aes(x = subject_id, y = OVERALL)) +
   geom_point( size = 4, color = "orange") +
   ylab("Overall Accuracy (%)") +
   coord_flip() +
   theme_bw() +
   lims(y = c(min(df_subject$OVERALL) - 0.5, 100)) +
   geom_label_repel(aes(label = paste0("id: ", subject_id)),
                  box.padding   = 0.35, 
                  point.padding = 0.5,
                  segment.color = 'grey50') +
   theme(axis.text.y=element_blank(),
         axis.title.y=element_blank(),
         text = element_text(size = 14))

ggsave(filename = "OverallAccBySubject.png", width = 14, height = 7, units = "in", bg = "white")

df_subject %>% 
   pivot_longer(cols = c(-subject_id), names_to = "Type", values_to = "Accuracy") %>% 
   group_by(Type, Accuracy_Rounded = round(Accuracy)) %>% 
   summarise(n = n()) %>% 
   pivot_wider(names_from = Type, values_from = n, values_fill = 0) %>% 
   arrange(desc(Accuracy_Rounded))
```


```{r}
df <- importance(rf_final) %>% 
   as.data.frame() %>% 
   rownames_to_column("feature") %>% 
   mutate(
      feature_type = case_when(
         substr(feature, 0, 2) == "ec" ~ "ECG",
         substr(feature, 0, 2) == "it" ~ "TEB",
         substr(feature, 0, 2) == "ed" ~ "EDA",
         TRUE ~ "ERROR"
      )
   ) %>% 
   mutate(feature = paste0(feature_type, " - ", feature))

df %>% 
   arrange(MeanDecreaseAccuracy) %>%
   tail(25) %>% 
   mutate(feature = factor(feature, levels = feature)) %>% 
   ggplot(aes(MeanDecreaseAccuracy, feature)) +
   geom_point() +
   scale_x_continuous(limits=c(0,NA), expand=expansion(c(0,0.04))) +
   theme_bw() +
   theme(panel.grid.minor=element_blank(),
         panel.grid.major.x=element_blank(),
         panel.grid.major.y=element_line(),
         axis.title=element_blank(),
         text = element_text(size = 14)) +
   labs(title = "Mean decrease in accuracy")

ggsave(filename = "VariableImportance.png", width = 14, height = 7, units = "in", bg = "white")

temp <- df %>% 
   arrange(MeanDecreaseAccuracy) %>%
   group_by(feature_type) %>% 
   summarise(n_all = n())

df %>% 
   arrange(MeanDecreaseAccuracy) %>%
   tail(25) %>% 
   group_by(feature_type) %>% 
   summarise(n = n()) %>% 
   left_join(temp, by = "feature_type") %>% 
   mutate(n_prop = n / n_all * 100)
```